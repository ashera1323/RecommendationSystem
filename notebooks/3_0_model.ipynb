{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dc6f7ec2022e41e1bb669024f6a263df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_09d34d0bc2ed49f799c4510e1fc4309d",
              "IPY_MODEL_c1460c0567714bcb976f146a4b484821",
              "IPY_MODEL_b3f0956476e046c9b8d19a9d6fdf2e59"
            ],
            "layout": "IPY_MODEL_dd4d8586a15543b0ab275b73dc093548"
          }
        },
        "09d34d0bc2ed49f799c4510e1fc4309d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8b51d67afe942e597588ec0fab5bf9a",
            "placeholder": "​",
            "style": "IPY_MODEL_cea6b46ef30847cfab01936e7f1aa078",
            "value": "100%"
          }
        },
        "c1460c0567714bcb976f146a4b484821": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9046ec457e94873a81d2b6d77f659fd",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_56ad298ccd1a4589a6f8258ae115b37d",
            "value": 10000
          }
        },
        "b3f0956476e046c9b8d19a9d6fdf2e59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_719f3021b0f54caeb8d7e5566a042b1b",
            "placeholder": "​",
            "style": "IPY_MODEL_059f95140c014ffeb82c7ff13bd51703",
            "value": " 10000/10000 [31:38&lt;00:00,  5.73it/s]"
          }
        },
        "dd4d8586a15543b0ab275b73dc093548": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8b51d67afe942e597588ec0fab5bf9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cea6b46ef30847cfab01936e7f1aa078": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9046ec457e94873a81d2b6d77f659fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56ad298ccd1a4589a6f8258ae115b37d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "719f3021b0f54caeb8d7e5566a042b1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "059f95140c014ffeb82c7ff13bd51703": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "U81VLYg7KHUe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2DZcRD2KDyW",
        "outputId": "f301f792-bbc8-46e8-9332-5271b2b0adfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.4.0\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.1.0+cpu.html\n",
            "Collecting pyg_lib\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcpu/pyg_lib-0.3.1%2Bpt21cpu-cp310-cp310-linux_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcpu/torch_scatter-2.1.2%2Bpt21cpu-cp310-cp310-linux_x86_64.whl (497 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.3/497.3 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcpu/torch_sparse-0.6.18%2Bpt21cpu-cp310-cp310-linux_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcpu/torch_cluster-1.6.3%2Bpt21cpu-cp310-cp310-linux_x86_64.whl (745 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m745.7/745.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_spline_conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcpu/torch_spline_conv-1.2.2%2Bpt21cpu-cp310-cp310-linux_x86_64.whl (208 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.5/208.5 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_sparse) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch_sparse) (1.23.5)\n",
            "Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\n",
            "Successfully installed pyg_lib-0.3.1+pt21cpu torch_cluster-1.6.3+pt21cpu torch_scatter-2.1.2+pt21cpu torch_sparse-0.6.18+pt21cpu torch_spline_conv-1.2.2+pt21cpu\n",
            "Requirement already satisfied: torch_sparse in /usr/local/lib/python3.10/dist-packages (0.6.18+pt21cpu)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_sparse) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch_sparse) (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric\n",
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.1.0+cpu.html\n",
        "!pip install torch_sparse\n",
        "\n",
        "# import required modules\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ctypes\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import model_selection, metrics, preprocessing\n",
        "import copy\n",
        "from torch_geometric.utils import degree\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim, Tensor\n",
        "\n",
        "from torch_sparse import SparseTensor, matmul\n",
        "\n",
        "from torch_geometric.utils import structured_negative_sampling\n",
        "from torch_geometric.data import download_url, extract_zip\n",
        "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.typing import Adj"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch"
      ],
      "metadata": {
        "id": "rnemxjfuKUhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function for training and compute BPR loss\n",
        "# since this is a self-supervised learning, we are relying on the graph structure itself and\n",
        "# we don't have label other than the graph structure so we need to the folloing function\n",
        "# which random samples a mini-batch of positive and negative samples\n",
        "def sample_mini_batch(batch_size, edge_index):\n",
        "    \"\"\"Randomly samples indices of a minibatch given an adjacency matrix\n",
        "\n",
        "    Args:\n",
        "        batch_size (int): minibatch size\n",
        "        edge_index (torch.Tensor): 2 by N list of edges\n",
        "\n",
        "    Returns:\n",
        "        tuple: user indices, positive item indices, negative item indices\n",
        "    \"\"\"\n",
        "    # structured_negative_sampling is a pyG library\n",
        "    # Samples a negative edge :obj:`(i,k)` for every positive edge\n",
        "    # :obj:`(i,j)` in the graph given by :attr:`edge_index`, and returns it as a\n",
        "    # tuple of the form :obj:`(i,j,k)`.\n",
        "    #\n",
        "    #         >>> edge_index = torch.as_tensor([[0, 0, 1, 2],\n",
        "    #         ...                               [0, 1, 2, 3]])\n",
        "    #         >>> structured_negative_sampling(edge_index)\n",
        "    #         (tensor([0, 0, 1, 2]), tensor([0, 1, 2, 3]), tensor([2, 3, 0, 2]))\n",
        "    edges = structured_negative_sampling(edge_index)\n",
        "\n",
        "    # 3 x edge_index_len\n",
        "    edges = torch.stack(edges, dim=0)\n",
        "\n",
        "    # here is whhen we actually perform the batch sampe\n",
        "    # Return a k sized list of population elements chosen with replacement.\n",
        "    indices = random.choices([i for i in range(edges[0].shape[0])], k=batch_size)\n",
        "\n",
        "    batch = edges[:, indices]\n",
        "\n",
        "    user_indices, pos_item_indices, neg_item_indices = batch[0], batch[1], batch[2]\n",
        "    return user_indices, pos_item_indices, neg_item_indices"
      ],
      "metadata": {
        "id": "TQkHROC_KaKn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing LightGCN\n",
        "\n",
        "## Light Graph Convolution\n",
        "Between each layer, LightGCN uses the following propagation rule for user and item embeddings.\n",
        "\n",
        "\\begin{equation}\n",
        "e_u^{(k+1)} = \\sum_{i \\in N_u} \\frac{1}{\\sqrt{|N_u|}\\sqrt{|N_i|}} e_i^{(k)} \\quad e_i^{(k+1)} = \\sum_{u \\in N_i} \\frac{1}{\\sqrt{|N_i|}\\sqrt{|N_u|}} e_u^{(k)}\n",
        "\\end{equation}\n",
        "\n",
        "$N_u$: the set of all neighbors of user $u$ (items liked by $u$)\n",
        "\n",
        "$N_i$: the set of all neighbors of item $i$ (users who liked $i$)\n",
        "\n",
        "$e_u^{(k)}$ : k-th layer user embedding\n",
        "\n",
        "$e_i^{(k)}$ : k-th layer item embedding\n",
        "\n",
        "\n",
        "\n",
        "## Layer Combination and Model Prediction\n",
        "The only trainable parameters of LightGCN are the 0-th layer embeddings $e_u^{(0)}$ and $e_i^{(0)}$ for each user and item. We combine the embeddings obtained at each layer of propagation to form the final embeddings for all user and item, $e_u$ and $e_i$ via the follwing equation.\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "e_u = \\sum_{k = 0}^K \\alpha_k e_u^{(k)} \\quad e_i = \\sum_{k = 0}^K \\alpha_k e_i^{(k)}\n",
        "\\end{equation}\n",
        "\n",
        "$\\alpha_k$ : hyperparameter which weights the contribution of the k-th layer embedding to the final embedding\n",
        "\n",
        "The model prediction is obtained by taking the inner product of the final user and item embeddings.\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{y}_{ui} = e_u^Te_i\n",
        "\\end{equation}\n",
        "\n",
        "## Matrix Form\n",
        "In our implementation, we utilize the matrix form of LightGCN. We perform multi-scale diffusion to obtain the final embedding, which sums embeddings diffused across multi-hop scales.\n",
        "\n",
        "\\begin{equation}\n",
        "E^{(K)} = \\alpha_0 E^{(0)} + \\alpha_1 \\tilde{A}^1 E^{(0)} + \\alpha_2 \\tilde{A}^2 E^{(0)} + \\cdot \\cdot \\cdot + \\alpha_K \\tilde{A}^K \\tilde{A} E^{(0)}\n",
        "\\end{equation}\n",
        "\n",
        "$E^{(0)} \\in \\mathcal{R}^{(M + N)} \\times T$ : stacked initial item and user embeddings where $M$, $N$, and $T$ denote the number of users, number of items, and the dimension of each embedding respectively\n",
        "\n",
        "$\\tilde{A} = D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$ : symmetrically normalized adjacency matrix\n"
      ],
      "metadata": {
        "id": "t9KyfovTKcl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defines LightGCN model\n",
        "class LightGCN(MessagePassing):\n",
        "    \"\"\"LightGCN Model as proposed in https://arxiv.org/abs/2002.02126\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_users,\n",
        "                 num_items,\n",
        "                 embedding_dim=64, # define the embding vector length for each node\n",
        "                 K=3,\n",
        "                 add_self_loops=False):\n",
        "        \"\"\"Initializes LightGCN Model\n",
        "\n",
        "        Args:\n",
        "            num_users (int): Number of users\n",
        "            num_items (int): Number of items\n",
        "            embedding_dim (int, optional): Dimensionality of embeddings. Defaults to 8.\n",
        "            K (int, optional): Number of message passing layers. Defaults to 3.\n",
        "            add_self_loops (bool, optional): Whether to add self loops for message passing. Defaults to False.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.K = K\n",
        "        self.add_self_loops = add_self_loops\n",
        "\n",
        "        # define user and item embedding for direct look up.\n",
        "        # embedding dimension: num_user/num_item x embedding_dim\n",
        "\n",
        "        self.users_emb = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.embedding_dim) # e_u^0\n",
        "\n",
        "        self.items_emb = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.embedding_dim) # e_i^0\n",
        "\n",
        "        # \"Fills the input Tensor with values drawn from the normal distribution\"\n",
        "        # according to LightGCN paper, this gives better performance\n",
        "        nn.init.normal_(self.users_emb.weight, std=0.1)\n",
        "        nn.init.normal_(self.items_emb.weight, std=0.1)\n",
        "\n",
        "    def forward(self, edge_index: Tensor):\n",
        "        \"\"\"Forward propagation of LightGCN Model.\n",
        "\n",
        "        Args:\n",
        "            edge_index (SparseTensor): adjacency matrix\n",
        "\n",
        "        Returns:\n",
        "            tuple (Tensor): e_u_k, e_u_0, e_i_k, e_i_0\n",
        "        \"\"\"\n",
        "\n",
        "        edge_index_norm = gcn_norm(edge_index=edge_index,\n",
        "                                   add_self_loops=self.add_self_loops)\n",
        "\n",
        "        # concat the user_emb and item_emb as the layer0 embing matrix\n",
        "        # size will be (n_users + n_items) x emb_vector_len.   e.g: 10334 x 64\n",
        "        emb_0 = torch.cat([self.users_emb.weight, self.items_emb.weight]) # E^0\n",
        "\n",
        "        embs = [emb_0] # save the layer0 emb to the embs list\n",
        "\n",
        "        # emb_k is the emb that we are actually going to push it through the graph layers\n",
        "        # as described in lightGCN paper formula 7\n",
        "        emb_k = emb_0\n",
        "\n",
        "        # push the embedding of all users and items through the Graph Model K times.\n",
        "        # K here is the number of layers\n",
        "        for i in range(self.K):\n",
        "            emb_k = self.propagate(edge_index=edge_index_norm[0], x=emb_k, norm=edge_index_norm[1])\n",
        "            embs.append(emb_k)\n",
        "\n",
        "\n",
        "        # this is doing the formula8 in LightGCN paper\n",
        "\n",
        "        # the stacked embs is a list of embedding matrix at each layer\n",
        "        #    it's of shape n_nodes x (n_layers + 1) x emb_vector_len.\n",
        "        #        e.g: torch.Size([10334, 4, 64])\n",
        "        embs = torch.stack(embs, dim=1)\n",
        "\n",
        "        # From LightGCn paper: \"In our experiments, we find that setting α_k uniformly as 1/(K + 1)\n",
        "        #    leads to good performance in general.\"\n",
        "        emb_final = torch.mean(embs, dim=1) # E^K\n",
        "\n",
        "\n",
        "        # splits into e_u^K and e_i^K\n",
        "        users_emb_final, items_emb_final = torch.split(emb_final, [self.num_users, self.num_items])\n",
        "\n",
        "        # returns e_u^K, e_u^0, e_i^K, e_i^0\n",
        "        # here using .weight to get the tensor weights from n.Embedding\n",
        "        return users_emb_final, self.users_emb.weight, items_emb_final, self.items_emb.weight\n",
        "\n",
        "    def message(self, x_j, norm):\n",
        "        # x_j is of shape:  edge_index_len x emb_vector_len\n",
        "        #    e.g: torch.Size([77728, 64]\n",
        "        #\n",
        "        # x_j is basically the embedding of all the neighbors based on the src_list in coo edge index\n",
        "        #\n",
        "        # elementwise multiply by the symmetrically norm. So it's essentiall what formula 7 in LightGCN\n",
        "        # paper does but here we are using edge_index rather than Adj Matrix\n",
        "        return norm.view(-1, 1) * x_j"
      ],
      "metadata": {
        "id": "SX_cz700Kfq8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layers = 3\n",
        "num_users = 943\n",
        "num_movies = 1682\n",
        "model = LightGCN(num_users=num_users,\n",
        "                 num_items=num_movies,\n",
        "                 K=layers)"
      ],
      "metadata": {
        "id": "21KXerV2Kj4-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Function\n",
        "\n",
        "\n",
        "\n",
        "We utilize a Bayesian Personalized Ranking (BPR) loss, a pairwise objective which encourages the predictions of positive samples to be higher than negative samples for each user.\n",
        "\n",
        "\\begin{equation}\n",
        "L_{BPR} = -\\sum_{u = 1}^M \\sum_{i \\in N_u} \\sum_{j \\notin N_u} \\ln{\\sigma(\\hat{y}_{ui} - \\hat{y}_{uj})} + \\lambda ||E^{(0)}||^2\n",
        "\\end{equation}\n",
        "\n",
        "$\\hat{y}_{u}$: predicted score of a positive sample\n",
        "\n",
        "$\\hat{y}_{uj}$: predicted score of a negative sample\n",
        "\n",
        "$\\lambda$: hyperparameter which controls the L2 regularization strength"
      ],
      "metadata": {
        "id": "sqCD8wVNLGAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bpr_loss(users_emb_final,\n",
        "             users_emb_0,\n",
        "             pos_items_emb_final,\n",
        "             pos_items_emb_0,\n",
        "             neg_items_emb_final,\n",
        "             neg_items_emb_0,\n",
        "             lambda_val):\n",
        "    \"\"\"Bayesian Personalized Ranking Loss as described in https://arxiv.org/abs/1205.2618\n",
        "\n",
        "    Args:\n",
        "        users_emb_final (torch.Tensor): e_u_k\n",
        "        users_emb_0 (torch.Tensor): e_u_0\n",
        "        pos_items_emb_final (torch.Tensor): positive e_i_k\n",
        "        pos_items_emb_0 (torch.Tensor): positive e_i_0\n",
        "        neg_items_emb_final (torch.Tensor): negative e_i_k\n",
        "        neg_items_emb_0 (torch.Tensor): negative e_i_0\n",
        "        lambda_val (float): lambda value for regularization loss term\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: scalar bpr loss value\n",
        "    \"\"\"\n",
        "    reg_loss = lambda_val * (users_emb_0.norm(2).pow(2) +\n",
        "                             pos_items_emb_0.norm(2).pow(2) +\n",
        "                             neg_items_emb_0.norm(2).pow(2)) # L2 loss\n",
        "\n",
        "    pos_scores = torch.mul(users_emb_final, pos_items_emb_final)\n",
        "    pos_scores = torch.sum(pos_scores, dim=-1) # predicted scores of positive samples\n",
        "    neg_scores = torch.mul(users_emb_final, neg_items_emb_final)\n",
        "    neg_scores = torch.sum(neg_scores, dim=-1) # predicted scores of negative samples\n",
        "\n",
        "\n",
        "    bpr_loss = -torch.mean(torch.nn.functional.softplus(pos_scores - neg_scores))\n",
        "\n",
        "    loss = bpr_loss + reg_loss\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "em_aEPqiLEqt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Metrics\n",
        "\n",
        "We evalaluate our model using the following metrics\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{Recall} = \\frac{TP}{TP + FP}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{Precision} = \\frac{TP}{TP + FN}\n",
        "\\end{equation}\n",
        "\n",
        "Recall@k and Precision@k is just applying only the topK recommended items and then for the overall\n",
        "Recall@k and Precision@k, it's just averaged by the number of users\n",
        "\n",
        "**Dicounted Cumulative Gain (DCG)** at rank position p is defined as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{DCG}_\\text{p} = \\sum_{i = 1}^p \\frac{2^{rel_i} - 1}{\\log_2{(i + 1)}}\n",
        "\\end{equation}\n",
        "\n",
        "p: a particular rank position\n",
        "\n",
        "$rel_i \\in \\{0, 1\\}$ : graded relevance of the result at position $i$\n",
        "\n",
        "**Idealised Dicounted Cumulative Gain (IDCG)**, namely the maximum possible DCG, at rank position $p$ is defined as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{IDCG}_\\text{p} = \\sum_{i = 1}^{|REL_p|} \\frac{2^{rel_i} - 1}{\\log_2{(i + 1)}}\n",
        "\\end{equation}\n",
        "\n",
        "$|REL_p|$ : list of items ordered by their relevance up to position p\n",
        "\n",
        "**Normalized Dicounted Cumulative Gain (NDCG)** at rank position $p$ is defined as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{nDCG}_\\text{p} = \\frac{\\text{DCG}_p}{\\text{nDCG}_p}\n",
        "\\end{equation}\n",
        "\n",
        "Specifically, we use the metrics recall@K, precision@K, and NDCG@K. @K indicates that these metrics are computed on the top K recommendations."
      ],
      "metadata": {
        "id": "VcZTY6k_PXKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_positive_items(edge_index):\n",
        "    \"\"\"\n",
        "    Generates dictionary of positive items for each user\n",
        "\n",
        "    Args:\n",
        "        edge_index (torch.Tensor): 2 by N list of edges\n",
        "\n",
        "    Returns:\n",
        "        dict: user -> list of positive items for each\n",
        "    \"\"\"\n",
        "\n",
        "    # key: user_id, val: item_id list\n",
        "    user_pos_items = {}\n",
        "\n",
        "    for i in range(edge_index.shape[1]):\n",
        "        user = edge_index[0][i].item()\n",
        "        item = edge_index[1][i].item()\n",
        "\n",
        "        if user not in user_pos_items:\n",
        "            user_pos_items[user] = []\n",
        "\n",
        "        user_pos_items[user].append(item)\n",
        "\n",
        "    return user_pos_items"
      ],
      "metadata": {
        "id": "wsRap-CfPf2I"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# computes recall@K and precision@K\n",
        "def RecallPrecision_ATk(groundTruth, r, k):\n",
        "    \"\"\"Computers recall @ k and precision @ k\n",
        "\n",
        "    Args:\n",
        "        groundTruth (list[list[long]]): list of lists of item_ids. Cntaining highly rated items of each user.\n",
        "                            In other words, this is the list of true_relevant_items for each user\n",
        "\n",
        "        r (list[list[boolean]]): list of lists indicating whether each top k item recommended to each user\n",
        "                            is a top k ground truth (true relevant) item or not\n",
        "\n",
        "        k (int): determines the top k items to compute precision and recall on\n",
        "\n",
        "    Returns:\n",
        "        tuple: recall @ k, precision @ k\n",
        "    \"\"\"\n",
        "\n",
        "    # number of correctly predicted items per user\n",
        "    # -1 here means I want to sum at the inner most dimension\n",
        "    num_correct_pred = torch.sum(r, dim=-1)\n",
        "\n",
        "    # number of items liked by each user in the test set\n",
        "    user_num_liked = torch.Tensor([len(groundTruth[i]) for i in range(len(groundTruth))])\n",
        "\n",
        "    recall = torch.mean(num_correct_pred / user_num_liked)\n",
        "    precision = torch.mean(num_correct_pred) / k\n",
        "    return recall.item(), precision.item()"
      ],
      "metadata": {
        "id": "3xVdMEfUPjpP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# computes NDCG@K\n",
        "def NDCGatK_r(groundTruth, r, k):\n",
        "    \"\"\"Computes Normalized Discounted Cumulative Gain (NDCG) @ k\n",
        "\n",
        "    Args:\n",
        "        groundTruth (list): list of lists containing highly rated items of each user\n",
        "        r (list): list of lists indicating whether each top k item recommended to each user\n",
        "            is a top k ground truth item or not\n",
        "        k (int): determines the top k items to compute ndcg on\n",
        "\n",
        "    Returns:\n",
        "        float: ndcg @ k\n",
        "    \"\"\"\n",
        "    assert len(r) == len(groundTruth)\n",
        "\n",
        "    test_matrix = torch.zeros((len(r), k))\n",
        "\n",
        "    for i, items in enumerate(groundTruth):\n",
        "        length = min(len(items), k)\n",
        "        test_matrix[i, :length] = 1\n",
        "    max_r = test_matrix\n",
        "    idcg = torch.sum(max_r * 1. / torch.log2(torch.arange(2, k + 2)), axis=1)\n",
        "    dcg = r * (1. / torch.log2(torch.arange(2, k + 2)))\n",
        "    dcg = torch.sum(dcg, axis=1)\n",
        "    idcg[idcg == 0.] = 1.\n",
        "    ndcg = dcg / idcg\n",
        "    ndcg[torch.isnan(ndcg)] = 0.\n",
        "    return torch.mean(ndcg).item()"
      ],
      "metadata": {
        "id": "U4W4qPR3PmMv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wrapper function to get evaluation metrics\n",
        "def get_metrics(model,\n",
        "                input_edge_index, # adj_mat based edge index\n",
        "                input_exclude_edge_indices, # adj_mat based exclude edge index\n",
        "                k):\n",
        "    \"\"\"Computes the evaluation metrics: recall, precision, and ndcg @ k\n",
        "\n",
        "    Args:\n",
        "        model (LighGCN): lightgcn model\n",
        "\n",
        "        edge_index (torch.Tensor): 2 by N list of edges for split to evaluate\n",
        "\n",
        "        exclude_edge_indices ([type]): 2 by N list of edges for split to discount from evaluation\n",
        "\n",
        "        k (int): determines the top k items to compute metrics on\n",
        "\n",
        "    Returns:\n",
        "        tuple: recall @ k, precision @ k, ndcg @ k\n",
        "    \"\"\"\n",
        "    # get the embedding tensor at layer 0 after training\n",
        "    user_embedding = model.users_emb.weight\n",
        "    item_embedding = model.items_emb.weight\n",
        "\n",
        "\n",
        "    # convert adj_mat based edge index to r_mat based edge index so we have have\n",
        "    # the first list being user_ids and second list being item_ids for the edge index\n",
        "    edge_index = convert_adj_mat_edge_index_to_r_mat_edge_index(input_edge_index)\n",
        "\n",
        "    # This is to exclude the edges we have seen before in our predicted interaction matrix (r_mat_rating)\n",
        "    # E.g: for validation set, we want want to exclude all the edges in training set\n",
        "    exclude_edge_indices = [convert_adj_mat_edge_index_to_r_mat_edge_index(exclude_edge_index) \\\n",
        "                                      for exclude_edge_index in input_exclude_edge_indices]\n",
        "\n",
        "\n",
        "\n",
        "    # Generate predicted interaction matrix (r_mat_rating)\n",
        "    # (num_users x 64) dot_product (num_item x 64).T\n",
        "    r_mat_rating = torch.matmul(user_embedding, item_embedding.T)\n",
        "\n",
        "    # shape: num_users x num_item\n",
        "    rating = r_mat_rating\n",
        "\n",
        "    for exclude_edge_index in exclude_edge_indices:\n",
        "        # gets all the positive items for each user from the edge index\n",
        "        # it's a dict: user -> positive item list\n",
        "        user_pos_items = get_user_positive_items(exclude_edge_index)\n",
        "\n",
        "        # get coordinates of all edges to exclude\n",
        "        exclude_users = []\n",
        "        exclude_items = []\n",
        "        for user, items in user_pos_items.items():\n",
        "            # [user] * len(item) can give us [user1, user1, user1...] with len of len(item)\n",
        "            # this makes easier to do the masking below\n",
        "            exclude_users.extend([user] * len(items))\n",
        "            exclude_items.extend(items)\n",
        "\n",
        "        # set the excluded entry in the rat_mat_rating matrix to a very small number\n",
        "        rating[exclude_users, exclude_items] = -(1 << 10)\n",
        "\n",
        "    # get the top k recommended items for each user\n",
        "    _, top_K_items = torch.topk(rating, k=k)\n",
        "\n",
        "    # get all unique users in evaluated split\n",
        "    users = edge_index[0].unique()\n",
        "\n",
        "    # dict of user -> pos_item list\n",
        "    test_user_pos_items = get_user_positive_items(edge_index)\n",
        "\n",
        "    # convert test user pos items dictionary into a list of lists\n",
        "    test_user_pos_items_list = [test_user_pos_items[user.item()] for user in users]\n",
        "\n",
        "\n",
        "    # r here is \"pred_relevant_items ∩ actually_relevant_items\" list for each user\n",
        "    r = []\n",
        "    for user in users:\n",
        "        user_true_relevant_item = test_user_pos_items[user.item()]\n",
        "        # list of Booleans to store whether or not a given item in the top_K_items for a given user\n",
        "        # is also present in user_true_relevant_item.\n",
        "        # this is later on used to compute n_rel_and_rec_k\n",
        "        label = list(map(lambda x: x in user_true_relevant_item, top_K_items[user]))\n",
        "        r.append(label)\n",
        "\n",
        "    r = torch.Tensor(np.array(r).astype('float'))\n",
        "\n",
        "    recall, precision = RecallPrecision_ATk(test_user_pos_items_list, r, k)\n",
        "    ndcg = NDCGatK_r(test_user_pos_items_list, r, k)\n",
        "\n",
        "    return recall, precision, ndcg"
      ],
      "metadata": {
        "id": "_dc0h5sKPqMV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wrapper function to evaluate model\n",
        "def evaluation(model,\n",
        "               edge_index, # adj_mat based edge index\n",
        "               exclude_edge_indices,  # adj_mat based exclude edge index\n",
        "               k,\n",
        "               lambda_val\n",
        "              ):\n",
        "    \"\"\"Evaluates model loss and metrics including recall, precision, ndcg @ k\n",
        "\n",
        "    Args:\n",
        "        model (LighGCN): lightgcn model\n",
        "        edge_index (torch.Tensor): 2 by N list of edges for split to evaluate\n",
        "        sparse_edge_index (sparseTensor): sparse adjacency matrix for split to evaluate\n",
        "        exclude_edge_indices ([type]): 2 by N list of edges for split to discount from evaluation\n",
        "        k (int): determines the top k items to compute metrics on\n",
        "        lambda_val (float): determines lambda for bpr loss\n",
        "\n",
        "    Returns:\n",
        "        tuple: bpr loss, recall @ k, precision @ k, ndcg @ k\n",
        "    \"\"\"\n",
        "    # get embeddings\n",
        "    users_emb_final, users_emb_0, items_emb_final, items_emb_0 = model.forward(edge_index)\n",
        "\n",
        "    r_mat_edge_index = convert_adj_mat_edge_index_to_r_mat_edge_index(edge_index)\n",
        "\n",
        "    edges = structured_negative_sampling(r_mat_edge_index, contains_neg_self_loops=False)\n",
        "\n",
        "    user_indices, pos_item_indices, neg_item_indices = edges[0], edges[1], edges[2]\n",
        "\n",
        "    users_emb_final, users_emb_0 = users_emb_final[user_indices], users_emb_0[user_indices]\n",
        "\n",
        "    pos_items_emb_final, pos_items_emb_0 = items_emb_final[pos_item_indices], items_emb_0[pos_item_indices]\n",
        "\n",
        "    neg_items_emb_final, neg_items_emb_0 = items_emb_final[neg_item_indices], items_emb_0[neg_item_indices]\n",
        "\n",
        "    loss = bpr_loss(users_emb_final,\n",
        "                    users_emb_0,\n",
        "                    pos_items_emb_final,\n",
        "                    pos_items_emb_0,\n",
        "                    neg_items_emb_final,\n",
        "                    neg_items_emb_0,\n",
        "                    lambda_val).item()\n",
        "\n",
        "\n",
        "    recall, precision, ndcg = get_metrics(model,\n",
        "                                          edge_index,\n",
        "                                          exclude_edge_indices,\n",
        "                                          k)\n",
        "\n",
        "    return loss, recall, precision, ndcg"
      ],
      "metadata": {
        "id": "f5nDAGwzPs4A"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "*Recall@K: 0.13, Precision@K: 0.045, NDCG@K: 0.10*"
      ],
      "metadata": {
        "id": "1KFfoRy-LZ6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define contants\n",
        "ITERATIONS = 10000\n",
        "EPOCHS = 10\n",
        "# ITERATIONS = 500\n",
        "BATCH_SIZE = 1024\n",
        "LR = 1e-3\n",
        "ITERS_PER_EVAL = 200\n",
        "ITERS_PER_LR_DECAY = 200\n",
        "K = 20\n",
        "LAMBDA = 1e-6\n",
        "# LAMBDA = 1/2"
      ],
      "metadata": {
        "id": "meIdJn91LZmm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device {device}.\")\n",
        "\n",
        "\n",
        "model = model.to(device)\n",
        "model.train()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "edge_index = torch.load('../data/interim/test_edge_index.pt')\n",
        "train_edge_index = torch.load('../data/interim/train_edge_index.pt')\n",
        "val_edge_index = torch.load('../data/interim/val_edge_index.pt')\n",
        "\n",
        "edge_index = edge_index.to(device)\n",
        "train_edge_index = train_edge_index.to(device)\n",
        "val_edge_index = val_edge_index.to(device)"
      ],
      "metadata": {
        "id": "scUUSimLLgMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_adj_mat_edge_index_to_r_mat_edge_index(input_edge_index):\n",
        "    sparse_input_edge_index = SparseTensor(row=input_edge_index[0],\n",
        "                                           col=input_edge_index[1],\n",
        "                                           sparse_sizes=((num_users + num_movies), num_users + num_movies))\n",
        "    adj_mat = sparse_input_edge_index.to_dense()\n",
        "    interact_mat = adj_mat[: num_users, num_users :]\n",
        "    r_mat_edge_index = interact_mat.to_sparse_coo().indices()\n",
        "    return r_mat_edge_index"
      ],
      "metadata": {
        "id": "aDI4piSjPKsr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embs_for_bpr(model, input_edge_index):\n",
        "    users_emb_final, users_emb_0, items_emb_final, items_emb_0 = model.forward(input_edge_index)\n",
        "\n",
        "\n",
        "    edge_index_to_use = convert_adj_mat_edge_index_to_r_mat_edge_index(input_edge_index)\n",
        "\n",
        "    # mini batching for eval and calculate loss\n",
        "    user_indices, pos_item_indices, neg_item_indices = sample_mini_batch(BATCH_SIZE, edge_index_to_use)\n",
        "\n",
        "    # This is to push tensor to device so if we are using GPU\n",
        "    user_indices, pos_item_indices, neg_item_indices = user_indices.to(device), pos_item_indices.to(device), neg_item_indices.to(device)\n",
        "\n",
        "\n",
        "    # we need layer0 embeddings and the final embeddings (computed from 0...K layer) for BPR loss computing\n",
        "    users_emb_final, users_emb_0 = users_emb_final[user_indices], users_emb_0[user_indices]\n",
        "    pos_items_emb_final, pos_items_emb_0 = items_emb_final[pos_item_indices], items_emb_0[pos_item_indices]\n",
        "    neg_items_emb_final, neg_items_emb_0 = items_emb_final[neg_item_indices], items_emb_0[neg_item_indices]\n",
        "\n",
        "    return users_emb_final, users_emb_0, pos_items_emb_final, pos_items_emb_0, neg_items_emb_final, neg_items_emb_0"
      ],
      "metadata": {
        "id": "biZilAhBO7rJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_recall_at_ks = []\n",
        "\n",
        "for iter in tqdm(range(ITERATIONS)):\n",
        "    # forward propagation\n",
        "    users_emb_final, users_emb_0,  pos_items_emb_final, pos_items_emb_0, neg_items_emb_final, neg_items_emb_0 \\\n",
        "                = get_embs_for_bpr(model, train_edge_index)\n",
        "\n",
        "    # loss computation\n",
        "    train_loss = bpr_loss(users_emb_final,\n",
        "                          users_emb_0,\n",
        "                          pos_items_emb_final,\n",
        "                          pos_items_emb_0,\n",
        "                          neg_items_emb_final,\n",
        "                          neg_items_emb_0,\n",
        "                          LAMBDA)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # validation set\n",
        "    if iter % ITERS_PER_EVAL == 0:\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_loss, recall, precision, ndcg = evaluation(model,\n",
        "                                                           val_edge_index,\n",
        "                                                           [train_edge_index],\n",
        "                                                           K,\n",
        "                                                           LAMBDA\n",
        "                                                          )\n",
        "\n",
        "            print(f\"[Iteration {iter}/{ITERATIONS}] train_loss: {round(train_loss.item(), 5)}, val_loss: {round(val_loss, 5)}, val_recall@{K}: {round(recall, 5)}, val_precision@{K}: {round(precision, 5)}, val_ndcg@{K}: {round(ndcg, 5)}\")\n",
        "\n",
        "            train_losses.append(train_loss.item())\n",
        "            val_losses.append(val_loss)\n",
        "            val_recall_at_ks.append(round(recall, 5))\n",
        "        model.train()\n",
        "\n",
        "    if iter % ITERS_PER_LR_DECAY == 0 and iter != 0:\n",
        "        scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 962,
          "referenced_widgets": [
            "dc6f7ec2022e41e1bb669024f6a263df",
            "09d34d0bc2ed49f799c4510e1fc4309d",
            "c1460c0567714bcb976f146a4b484821",
            "b3f0956476e046c9b8d19a9d6fdf2e59",
            "dd4d8586a15543b0ab275b73dc093548",
            "e8b51d67afe942e597588ec0fab5bf9a",
            "cea6b46ef30847cfab01936e7f1aa078",
            "c9046ec457e94873a81d2b6d77f659fd",
            "56ad298ccd1a4589a6f8258ae115b37d",
            "719f3021b0f54caeb8d7e5566a042b1b",
            "059f95140c014ffeb82c7ff13bd51703"
          ]
        },
        "id": "WqXE5XdVPMeF",
        "outputId": "d11d1f4b-0df4-44dc-db62-d3a6a1324615"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc6f7ec2022e41e1bb669024f6a263df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Iteration 0/10000] train_loss: -6.43683, val_loss: -6.06564, val_recall@20: 0.09669, val_precision@20: 0.03117, val_ndcg@20: 0.06606\n",
            "[Iteration 200/10000] train_loss: -22.57625, val_loss: -22.52425, val_recall@20: 0.10752, val_precision@20: 0.03479, val_ndcg@20: 0.07074\n",
            "[Iteration 400/10000] train_loss: -47.70509, val_loss: -46.19606, val_recall@20: 0.11608, val_precision@20: 0.03703, val_ndcg@20: 0.07717\n",
            "[Iteration 600/10000] train_loss: -79.31852, val_loss: -74.30136, val_recall@20: 0.11764, val_precision@20: 0.03915, val_ndcg@20: 0.08153\n",
            "[Iteration 800/10000] train_loss: -114.41795, val_loss: -107.76845, val_recall@20: 0.11786, val_precision@20: 0.04047, val_ndcg@20: 0.08287\n",
            "[Iteration 1000/10000] train_loss: -153.88362, val_loss: -143.35614, val_recall@20: 0.12318, val_precision@20: 0.04168, val_ndcg@20: 0.08524\n",
            "[Iteration 1200/10000] train_loss: -191.73981, val_loss: -179.71063, val_recall@20: 0.12559, val_precision@20: 0.04196, val_ndcg@20: 0.0878\n",
            "[Iteration 1400/10000] train_loss: -230.88203, val_loss: -217.79034, val_recall@20: 0.12495, val_precision@20: 0.04196, val_ndcg@20: 0.08717\n",
            "[Iteration 1600/10000] train_loss: -254.32043, val_loss: -260.93915, val_recall@20: 0.12911, val_precision@20: 0.04277, val_ndcg@20: 0.08831\n",
            "[Iteration 1800/10000] train_loss: -309.96762, val_loss: -294.05368, val_recall@20: 0.12635, val_precision@20: 0.04259, val_ndcg@20: 0.08765\n",
            "[Iteration 2000/10000] train_loss: -345.3382, val_loss: -333.94476, val_recall@20: 0.12613, val_precision@20: 0.04357, val_ndcg@20: 0.08892\n",
            "[Iteration 2200/10000] train_loss: -390.13593, val_loss: -371.89233, val_recall@20: 0.13737, val_precision@20: 0.04541, val_ndcg@20: 0.09313\n",
            "[Iteration 2400/10000] train_loss: -432.17331, val_loss: -412.31598, val_recall@20: 0.13333, val_precision@20: 0.04478, val_ndcg@20: 0.09273\n",
            "[Iteration 2600/10000] train_loss: -469.22092, val_loss: -446.38574, val_recall@20: 0.13107, val_precision@20: 0.04592, val_ndcg@20: 0.09324\n",
            "[Iteration 2800/10000] train_loss: -470.84637, val_loss: -488.8089, val_recall@20: 0.13318, val_precision@20: 0.04633, val_ndcg@20: 0.09425\n",
            "[Iteration 3000/10000] train_loss: -557.5354, val_loss: -522.61127, val_recall@20: 0.13614, val_precision@20: 0.04742, val_ndcg@20: 0.09609\n",
            "[Iteration 3200/10000] train_loss: -567.39758, val_loss: -559.50226, val_recall@20: 0.13819, val_precision@20: 0.04788, val_ndcg@20: 0.09706\n",
            "[Iteration 3400/10000] train_loss: -592.42157, val_loss: -589.92896, val_recall@20: 0.1433, val_precision@20: 0.04851, val_ndcg@20: 0.09883\n",
            "[Iteration 3600/10000] train_loss: -621.54889, val_loss: -612.15833, val_recall@20: 0.14548, val_precision@20: 0.04931, val_ndcg@20: 0.09892\n",
            "[Iteration 3800/10000] train_loss: -692.29913, val_loss: -646.06671, val_recall@20: 0.14742, val_precision@20: 0.04925, val_ndcg@20: 0.09902\n",
            "[Iteration 4000/10000] train_loss: -718.63519, val_loss: -677.41486, val_recall@20: 0.15211, val_precision@20: 0.04994, val_ndcg@20: 0.10119\n",
            "[Iteration 4200/10000] train_loss: -727.39893, val_loss: -709.10638, val_recall@20: 0.14822, val_precision@20: 0.0492, val_ndcg@20: 0.10058\n",
            "[Iteration 4400/10000] train_loss: -764.04919, val_loss: -743.20709, val_recall@20: 0.14831, val_precision@20: 0.04931, val_ndcg@20: 0.10157\n",
            "[Iteration 4600/10000] train_loss: -762.89984, val_loss: -763.73999, val_recall@20: 0.15057, val_precision@20: 0.05006, val_ndcg@20: 0.10214\n",
            "[Iteration 4800/10000] train_loss: -798.48199, val_loss: -788.09619, val_recall@20: 0.15151, val_precision@20: 0.05006, val_ndcg@20: 0.10281\n",
            "[Iteration 5000/10000] train_loss: -847.9538, val_loss: -818.65228, val_recall@20: 0.15149, val_precision@20: 0.05017, val_ndcg@20: 0.10292\n",
            "[Iteration 5200/10000] train_loss: -878.9649, val_loss: -838.49188, val_recall@20: 0.14932, val_precision@20: 0.04966, val_ndcg@20: 0.10205\n",
            "[Iteration 5400/10000] train_loss: -894.48413, val_loss: -863.487, val_recall@20: 0.14846, val_precision@20: 0.04937, val_ndcg@20: 0.10148\n",
            "[Iteration 5600/10000] train_loss: -882.64484, val_loss: -897.56177, val_recall@20: 0.15076, val_precision@20: 0.05006, val_ndcg@20: 0.1029\n",
            "[Iteration 5800/10000] train_loss: -918.4162, val_loss: -910.03735, val_recall@20: 0.1499, val_precision@20: 0.04966, val_ndcg@20: 0.10235\n",
            "[Iteration 6000/10000] train_loss: -914.13177, val_loss: -912.94867, val_recall@20: 0.15046, val_precision@20: 0.0496, val_ndcg@20: 0.10263\n",
            "[Iteration 6200/10000] train_loss: -992.84155, val_loss: -941.05334, val_recall@20: 0.15023, val_precision@20: 0.04966, val_ndcg@20: 0.10247\n",
            "[Iteration 6400/10000] train_loss: -1015.53943, val_loss: -967.41193, val_recall@20: 0.15038, val_precision@20: 0.05011, val_ndcg@20: 0.10305\n",
            "[Iteration 6600/10000] train_loss: -998.05487, val_loss: -971.24042, val_recall@20: 0.15031, val_precision@20: 0.04977, val_ndcg@20: 0.10313\n",
            "[Iteration 6800/10000] train_loss: -1047.87439, val_loss: -997.97876, val_recall@20: 0.153, val_precision@20: 0.05063, val_ndcg@20: 0.10418\n",
            "[Iteration 7000/10000] train_loss: -1025.9707, val_loss: -999.05017, val_recall@20: 0.15113, val_precision@20: 0.05029, val_ndcg@20: 0.10365\n",
            "[Iteration 7200/10000] train_loss: -1033.49841, val_loss: -1001.00299, val_recall@20: 0.15183, val_precision@20: 0.05029, val_ndcg@20: 0.10376\n",
            "[Iteration 7400/10000] train_loss: -1074.89697, val_loss: -1041.57422, val_recall@20: 0.15146, val_precision@20: 0.05011, val_ndcg@20: 0.10366\n",
            "[Iteration 7600/10000] train_loss: -1076.82788, val_loss: -1049.09033, val_recall@20: 0.15123, val_precision@20: 0.05046, val_ndcg@20: 0.10417\n",
            "[Iteration 7800/10000] train_loss: -1123.04749, val_loss: -1051.58691, val_recall@20: 0.15126, val_precision@20: 0.05017, val_ndcg@20: 0.10395\n",
            "[Iteration 8000/10000] train_loss: -1126.53674, val_loss: -1068.62708, val_recall@20: 0.15162, val_precision@20: 0.05006, val_ndcg@20: 0.10385\n",
            "[Iteration 8200/10000] train_loss: -1163.69067, val_loss: -1083.44641, val_recall@20: 0.15027, val_precision@20: 0.04994, val_ndcg@20: 0.10363\n",
            "[Iteration 8400/10000] train_loss: -1162.2854, val_loss: -1098.53601, val_recall@20: 0.15341, val_precision@20: 0.05034, val_ndcg@20: 0.10462\n",
            "[Iteration 8600/10000] train_loss: -1146.29736, val_loss: -1107.45544, val_recall@20: 0.1516, val_precision@20: 0.05029, val_ndcg@20: 0.10433\n",
            "[Iteration 8800/10000] train_loss: -1101.68652, val_loss: -1104.51196, val_recall@20: 0.15248, val_precision@20: 0.05029, val_ndcg@20: 0.10434\n",
            "[Iteration 9000/10000] train_loss: -1261.72021, val_loss: -1134.67932, val_recall@20: 0.15301, val_precision@20: 0.05034, val_ndcg@20: 0.10455\n",
            "[Iteration 9200/10000] train_loss: -1213.64954, val_loss: -1125.68982, val_recall@20: 0.15331, val_precision@20: 0.0504, val_ndcg@20: 0.10461\n",
            "[Iteration 9400/10000] train_loss: -1163.0343, val_loss: -1156.44885, val_recall@20: 0.15364, val_precision@20: 0.05052, val_ndcg@20: 0.10492\n",
            "[Iteration 9600/10000] train_loss: -1196.66943, val_loss: -1138.65479, val_recall@20: 0.15431, val_precision@20: 0.05057, val_ndcg@20: 0.10518\n",
            "[Iteration 9800/10000] train_loss: -1163.20251, val_loss: -1157.41614, val_recall@20: 0.15482, val_precision@20: 0.05086, val_ndcg@20: 0.10522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'final-model.pt')"
      ],
      "metadata": {
        "id": "hCLtXwsUmW80"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}